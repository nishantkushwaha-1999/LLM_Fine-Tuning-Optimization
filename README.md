# LLM_Fine-Tuning-Optimization

This repo focuses on making the fine-tuning of LLMs esay. There is a wrapper that is developed on top of hugging face libraries that utilizes LoRA to fine tune the LLMs. This repo makes fine tuning so simple that it can be done by just selecting the model and the train files.

## Installation

### Cloning the Repository
To use the scripts, first clone the repository to your local machine:

```bash
!git clone https://github.com/nishantkushwaha-1999/llm_fine_tuning_optimization.git
```

### Installing Dependencies
After cloning, navigate to the directory and install the required Python packages:

```bash
pip install -r ./llm_fine_tuning_optimization/requirements.txt
```


## Usage
To start fine-tuning the models, ensure you have the necessary data and configurations set up, and run the Python scripts located in the repository. The main entry point is typically a script that orchestrates the fine-tuning process.

### Example Command
python
from llm_fine_tuning_optimization.Tunner import fine_tunner
# Example usage of fine_tunner function


## Contributing
Contributions to this repository are welcome. Please create a pull request with your proposed changes.

## License
This project is licensed under the MIT License - see the LICENSE file for details.

---

Would you like to add or modify any sections in this README?
